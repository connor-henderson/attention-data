{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Tuple\n",
    "from typing_extensions import Literal\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import webbrowser\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import circuitsvis as cv\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set API Keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "assert OPENAI_API_KEY, \"OPENAI_API_KEY environment variable is missing from .env\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "content = \"You are a helpful mechanistic interpretability researcher who is an expert in analyzing attention patterns\"\n",
    "# response = openai.ChatCompletion.create(model=self.model, messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": prompt}], max_tokens=100, tools=tools) # tools=tools\n",
    "#         response_message = response['choices'][0]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = \"gpt-3.5-turbo-16k\" # \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    d_model=768,\n",
    "    d_head=64,\n",
    "    n_heads=12,\n",
    "    n_layers=2,\n",
    "    n_ctx=2048,\n",
    "    d_vocab=50278,\n",
    "    attention_dir=\"causal\",\n",
    "    attn_only=True, # defaults to False\n",
    "    tokenizer_name=\"EleutherAI/gpt-neox-20b\", \n",
    "    seed=398,\n",
    "    use_attn_result=True,\n",
    "    normalization_type=None, # defaults to \"LN\", i.e. layernorm with weights & biases\n",
    "    positional_embedding_type=\"shortformer\"\n",
    ")\n",
    "\n",
    "weights_dir = \"test-heads/attn_only_2L_half.pth\"\n",
    "\n",
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = t.load(weights_dir, map_location=device)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 70])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].shape\n",
    "random_sentences = [\"From around 1242, he was co-ruler with his father, and his relationship with some prominent aristocrats grew tense.\", \n",
    "                    \"Lack of approval for a congressional delegation resulted in the ceremony being delayed from the scheduled time on 14 January to the very early morning of 15 January\", \n",
    "                    \"He came to Sweden with Cardinal Nicholas Breakspeare in 1153 and was most likely designated to be the new Archbishop of Uppsala, but the independent church province of Sweden could only be established in 1164 after the civil war, and Henry would have been sent to organize the Church in Finland, where Christians had already existed for two centuries.\", \n",
    "                    \"Bright vixens jump over the moon; dozy fowl quack at the shimmering lake.\", \n",
    "                    \"Sphinx of black quartz, judge my vow as I stand in the heart of the Egyptian desert.\"]\n",
    "input_ids = model.tokenizer(random_sentences, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "input_ids.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.\"\n",
    "\n",
    "# logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "\n",
    "# str_tokens = model.to_str_tokens(text)\n",
    "# for layer in range(model.cfg.n_layers):\n",
    "#     attention_pattern = cache[\"pattern\", layer]\n",
    "#     display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits, cache = model.run_with_cache(input_ids)\n",
    "# random_idx = 0\n",
    "# new_text = random_sentences[random_idx]\n",
    "\n",
    "# str_tokens = model.to_str_tokens(new_text)\n",
    "# for layer in range(model.cfg.n_layers):\n",
    "#     attention_pattern = cache[\"pattern\", 0][random_idx, ...]\n",
    "#     display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient(x):\n",
    "    x = x.flatten() #all values are treated equally, arrays must be 1d\n",
    "    n = x.shape[0]\n",
    "    x = x.sort()[0] #values must be sorted\n",
    "    index = t.arange(1, n+1, device=device) #index per array element\n",
    "    return (t.sum((2 * index - n  - 1) * x)) / (n * t.sum(x)) #Gini coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_row_idx(end_idx: int) -> int:\n",
    "    min_idx = 10 if end_idx > 10 else 1\n",
    "    max_idx = end_idx\n",
    "    return random.randint(min_idx, max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEWEST\n",
    "prompt_prefix = \"\"\"\n",
    "Here is a row by row break down of which tokens were attended to for each individual generation step for an autoregressive transformer.\n",
    "Each row is labeled with its index and is of the format for each row: `\\{text\\}\\\\n\\{{token} {multiple} for token, multiple in above_avg_tokens}`.\n",
    "Notably, the tokens with above average attention are sorted by their score's multiple relative to the average attention score for that row,\n",
    "and this multiple is included next to it, i.e. `tokenX 2, tokenY 1.5` etc.\n",
    "\"\"\"\n",
    "\n",
    "prompt_suffix = \"\"\"\n",
    "Please generate a response in the format `row \\{idx\\}: \\{your generated description of this row's pattern of attention here\\}` for each row.\n",
    "Be as specific as possible, making sure to consider all possible reasons certain tokens may have been attended to more than others, including \n",
    "the position of the tokens, what the tokens themselves represent, the attention scores, and how these all interact.\n",
    "Limit your description to 1-3 sentences per row.\n",
    "\"\"\"\n",
    "\n",
    "num_row_samples = 1\n",
    "\n",
    "def get_multiple(score, avg_score):\n",
    "    return round((score / avg_score).item(), 1)\n",
    "\n",
    "def get_attention_pattern_prompt_for_rows(cache: ActivationCache, layer: int, head: int, tokens: List[List[str]], pad_token: str) -> str:\n",
    "    attention_pattern = cache[\"pattern\", layer][:, head, ...]\n",
    "    n_ctx, batch_size = len(tokens[0]), len(tokens)\n",
    "    assert attention_pattern.shape == (batch_size, n_ctx, n_ctx), f\"The cached attention pattern shape {attention_pattern.shape} != tokens shape {(batch_size, n_ctx, n_ctx)}\"\n",
    "    \n",
    "    # Randomly sample up to 5 batch indices\n",
    "    sampled_batch_indices = random.sample(range(batch_size), min(5, batch_size))\n",
    "    \n",
    "    # Generate rows of string for each sampled index\n",
    "    rows = []\n",
    "    for sample_idx in sampled_batch_indices:  # Iterate over sampled batches\n",
    "        end_idx = n_ctx\n",
    "        if pad_token in tokens[sample_idx][1:]:\n",
    "            end_idx = tokens[sample_idx][1:].index(pad_token)\n",
    "        idx = get_random_row_idx(end_idx)\n",
    "        # Only consider tokens that the current token could attend to (itself and previous tokens)\n",
    "        num_tokens = idx + 1\n",
    "        relevant_tokens = tokens[sample_idx][:num_tokens]  # Adjust for batch index\n",
    "        text = \"\".join(relevant_tokens)\n",
    "        relevant_scores = attention_pattern[sample_idx, idx, :num_tokens]  # Adjust for batch index\n",
    "        avg_score = 1 / num_tokens\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        above_avg_tokens = [(token, get_multiple(score, avg_score)) for token, score in zip(relevant_tokens, relevant_scores) if score > avg_score]\n",
    "        above_avg_tokens.sort(key=lambda x: x[1], reverse=True)\n",
    "        above_avg_tokens = \", \".join([f\"{token} {multiple}\" for token, multiple in above_avg_tokens])\n",
    "        \n",
    "        row = f\"{text}\\n{above_avg_tokens}\\n\"\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    # Combine the sampled rows into a single string with line breaks\n",
    "    rows_str = \"\\n\".join(rows)\n",
    "    \n",
    "    # Combine the prefix, rows, and suffix into the final prompt\n",
    "    prompt = f\"{prompt_prefix}\\n{rows_str}\\n{prompt_suffix}\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random sentences\n",
    "selected_sentences = random.sample(text, 10)\n",
    "\n",
    "input_ids = model.tokenizer(selected_sentences, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "max_length = 40\n",
    "if input_ids.shape[1] > max_length:\n",
    "    input_ids = input_ids[:, :max_length]\n",
    "\n",
    "decoded = model.tokenizer.batch_decode(input_ids)\n",
    "str_tokens = model.to_str_tokens(decoded, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, new_cache = model.run_with_cache(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is a row by row break down of which tokens were attended to for each individual generation step for an autoregressive transformer.\n",
      "Each row is labeled with its index and is of the format for each row: `\\{text\\}\\n\\{{token} {multiple} for token, multiple in above_avg_tokens}`.\n",
      "Notably, the tokens with above average attention are sorted by their score's multiple relative to the average attention score for that row,\n",
      "and this multiple is included next to it, i.e. `tokenX 2, tokenY 1.5` etc.\n",
      "\n",
      "Truman Tells Stalin, July 24, 1945\n",
      "\n",
      "Most of\n",
      " Stalin 7.4,  1945 2.4\n",
      "\n",
      "MLB Hall of Fame catcher Ivan \"Pudge\" Rodriguez stands with Texas Rangers manager Jeff Banister (28) in the dugout before a spring\n",
      "ML 21.7,  before 1.4,  in 1.0\n",
      "\n",
      "NSA whistleblower Edward Snowden appeared via video link from Moscow at the CeBIT IT trade conference in\n",
      " Moscow 5.0,  Snowden 2.8,  conference 2.2,  trade 1.9, blower 1.5,  Edward 1.3, N 1.1\n",
      "\n",
      "Universal's film will be tied to the Caribbean area where several ships and planes have disappeared under mysterious circumstances over the years.\n",
      "\n",
      "Mr. Robot\n",
      "Universal 19.8, . 2.1, . 1.7, \n",
      " 1.7, \n",
      " 1.5\n",
      "\n",
      "Friend can suggest an interesting feature? He would not work a lot with Xposed, it would be with\n",
      "Friend 9.4,  He 1.7,  interesting 1.6,  lot 1.6,  X 1.5,  feature 1.0\n",
      "\n",
      "\n",
      "Please generate a response in the format `row \\{idx\\}: \\{your generated description of this row's pattern of attention here\\}` for each row.\n",
      "Be as specific as possible, making sure to consider all possible reasons certain tokens may have been attended to more than others, including \n",
      "the position of the tokens, what the tokens themselves represent, the attention scores, and how these all interact.\n",
      "Limit your description to 1-3 sentences per row.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = get_attention_pattern_prompt_for_rows(new_cache, 0, 2, str_tokens, model.tokenizer.pad_token)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'said', '\"', 'What', 'for', '?\"\\''], [\"'\", 'We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', \",'\", 'said', 'the', 'Mock', 'Turtle', 'angrily', ':', \"'\", 'really', 'you', 'are', 'very', 'dull', \"!'\"], ['cried', 'the', 'Gryphon', ',', 'and', ',', 'taking', 'Alice', 'by', 'the', 'hand', ',', 'it', 'hurried', 'off', ',', 'without', 'waiting', 'for', 'the', 'end', 'of', 'the', 'song', '.'], [\"'\", 'I', \"'\", 'm', 'glad', 'they', \"'\", 've', 'begun', 'asking', 'riddles', '.--', 'I', 'believe', 'I', 'can', 'guess', 'that', \",'\", 'she', 'added', 'aloud', '.'], [\"'\", 'And', 'ever', 'since', 'that', \",'\", 'the', 'Hatter', 'went', 'on', 'in', 'a', 'mournful', 'tone', ',', \"'\", 'he', 'won', \"'\", 't', 'do', 'a', 'thing', 'I', 'ask', '!']]\n",
      "[[['<|endoftext|>', 'I'], ['<|endoftext|>', 'said'], ['<|endoftext|>', '\"'], ['<|endoftext|>', 'What'], ['<|endoftext|>', 'for'], ['<|endoftext|>', '?\"', \"'\"]], [['<|endoftext|>', \"'\"], ['<|endoftext|>', 'We'], ['<|endoftext|>', 'called'], ['<|endoftext|>', 'him'], ['<|endoftext|>', 'T', 'ort', 'oise'], ['<|endoftext|>', 'because'], ['<|endoftext|>', 'he'], ['<|endoftext|>', 'ta', 'ught'], ['<|endoftext|>', 'us'], ['<|endoftext|>', \",'\"], ['<|endoftext|>', 'said'], ['<|endoftext|>', 'the'], ['<|endoftext|>', 'Mock'], ['<|endoftext|>', 'T', 'urtle'], ['<|endoftext|>', 'ang', 'r', 'ily'], ['<|endoftext|>', ':'], ['<|endoftext|>', \"'\"], ['<|endoftext|>', 'really'], ['<|endoftext|>', 'you'], ['<|endoftext|>', 'are'], ['<|endoftext|>', 'very'], ['<|endoftext|>', 'd', 'ull'], ['<|endoftext|>', \"!'\"]], [['<|endoftext|>', 'c', 'ried'], ['<|endoftext|>', 'the'], ['<|endoftext|>', 'G', 'ry', 'phon'], ['<|endoftext|>', ','], ['<|endoftext|>', 'and'], ['<|endoftext|>', ','], ['<|endoftext|>', 'taking'], ['<|endoftext|>', 'Al', 'ice'], ['<|endoftext|>', 'by'], ['<|endoftext|>', 'the'], ['<|endoftext|>', 'hand'], ['<|endoftext|>', ','], ['<|endoftext|>', 'it'], ['<|endoftext|>', 'hur', 'ried'], ['<|endoftext|>', 'off'], ['<|endoftext|>', ','], ['<|endoftext|>', 'without'], ['<|endoftext|>', 'wait', 'ing'], ['<|endoftext|>', 'for'], ['<|endoftext|>', 'the'], ['<|endoftext|>', 'end'], ['<|endoftext|>', 'of'], ['<|endoftext|>', 'the'], ['<|endoftext|>', 'song'], ['<|endoftext|>', '.']], [['<|endoftext|>', \"'\"], ['<|endoftext|>', 'I'], ['<|endoftext|>', \"'\"], ['<|endoftext|>', 'm'], ['<|endoftext|>', 'gl', 'ad'], ['<|endoftext|>', 'they'], ['<|endoftext|>', \"'\"], ['<|endoftext|>', 've'], ['<|endoftext|>', 'beg', 'un'], ['<|endoftext|>', 'ask', 'ing'], ['<|endoftext|>', 'r', 'idd', 'les'], ['<|endoftext|>', '.--'], ['<|endoftext|>', 'I'], ['<|endoftext|>', 'believe'], ['<|endoftext|>', 'I'], ['<|endoftext|>', 'can'], ['<|endoftext|>', 'gu', 'ess'], ['<|endoftext|>', 'that'], ['<|endoftext|>', \",'\"], ['<|endoftext|>', 'she'], ['<|endoftext|>', 'added'], ['<|endoftext|>', 'al', 'oud'], ['<|endoftext|>', '.']], [['<|endoftext|>', \"'\"], ['<|endoftext|>', 'And'], ['<|endoftext|>', 'ever'], ['<|endoftext|>', 'since'], ['<|endoftext|>', 'that'], ['<|endoftext|>', \",'\"], ['<|endoftext|>', 'the'], ['<|endoftext|>', 'H', 'atter'], ['<|endoftext|>', 'went'], ['<|endoftext|>', 'on'], ['<|endoftext|>', 'in'], ['<|endoftext|>', 'a'], ['<|endoftext|>', 'm', 'our', 'n', 'ful'], ['<|endoftext|>', 'tone'], ['<|endoftext|>', ','], ['<|endoftext|>', \"'\"], ['<|endoftext|>', 'he'], ['<|endoftext|>', 'won'], ['<|endoftext|>', \"'\"], ['<|endoftext|>', 't'], ['<|endoftext|>', 'do'], ['<|endoftext|>', 'a'], ['<|endoftext|>', 'thing'], ['<|endoftext|>', 'I'], ['<|endoftext|>', 'ask'], ['<|endoftext|>', '!']]]\n"
     ]
    }
   ],
   "source": [
    "# num_samples = 5\n",
    "# random_indices = random.sample(range(len(sentences)), num_samples)\n",
    "# random_sentences = [sentences[i] for i in random_indices]\n",
    "# random_sentences_str_toks = [model.to_str_tokens(sentence) for sentence in random_sentences]\n",
    "# print(random_sentences)\n",
    "# print(random_sentences_str_toks)\n",
    "\n",
    "\n",
    "# selected_sentences = random.sample(list(sentences), 5)\n",
    "# sentences\n",
    "# input_ids = model.tokenizer(selected_sentences, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "# print(input_ids)\n",
    "# str_tokens = [model.to_str_tokens(sentence) for sentence in input_ids]\n",
    "# prompt = get_attention_pattern_prompt_for_rows(cache, 0, 0, str_tokens, model.tokenizer.pad_token)\n",
    "# print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = model.tokenizer(random_sentences, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "str_tokens = [model.to_str_tokens(sentence) for sentence in input_ids]\n",
    "prompt = get_attention_pattern_prompt_for_rows(cache, 0, 0, str_tokens, model.tokenizer.pad_token)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Invalid tokens input to to_str_tokens, has shape: torch.Size([5, 70])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer(random_sentences, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m tokenized_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_str_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m tokenized_sentences\n",
      "File \u001b[0;32m~/Desktop/mindful/env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:868\u001b[0m, in \u001b[0;36mHookedTransformer.to_str_tokens\u001b[0;34m(self, input, prepend_bos, padding_side)\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Don't pass dimensionless tensor\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 868\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    869\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid tokens input to to_str_tokens, has shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    871\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Invalid tokens input to to_str_tokens, has shape: torch.Size([5, 70])"
     ]
    }
   ],
   "source": [
    "input_ids = model.tokenizer(random_sentences, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "tokenized_sentences = model.to_str_tokens(input_ids)\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 59: The attention is spread across multiple tokens, mostly focusing on the words discussing the potential creation of powerful machine intelligence and the consequences of scaling up current machine learning techniques. The highest attention scores are observed for tokens related to the idea that systems would be deceptive or manipulative without solid plans to avoid such behavior.\n",
      "\n",
      "row 25: Attention spans across various tokens, particularly focusing on the discussion of the likelihood of powerful machine intelligence being created in this century. Attention weights are highest for tokens related to the possibility of current machine learning techniques leading to this outcome.\n",
      "\n",
      "row 27: Attention mostly concentrates on tokens related to the discussion of powerful machine intelligence being created and the assessment of the likelihood of this occurrence. The highest attention scores are observed for tokens discussing the consequences of scaling up current machine learning techniques.\n",
      "\n",
      "row 15: Attention is primarily focused on tokens pertaining to the belief that powerful machine intelligence is more likely than not to be created. The highest attention scores are associated with tokens indicating the strong likelihood of this outcome.\n",
      "\n",
      "row 38: Attention is spread across various tokens, particularly focusing on the discussion of powerful machine intelligence being created in this century and the consequences of scaling up current machine learning techniques. The highest attention scores are observed for tokens discussing the possibility of systems being deceptive or manipulative.\n",
      "\n",
      "row 28: The attention predominantly focuses on tokens related to the likelihood of powerful machine intelligence being created in this century. Attention weights are highest for tokens discussing the consequences of scaling up current machine learning techniques.\n",
      "\n",
      "row 23: Attention is spread across various tokens, emphasizing the discussion of powerful machine intelligence being more likely than not to be created in this century. The highest attention scores are associated with tokens discussing the consequences of this outcome.\n",
      "\n",
      "row 29: The attention is concentrated on tokens related to the likelihood of powerful machine intelligence being created and the assessment of this likelihood. The highest attention scores are observed for tokens discussing the consequences of scaling up current machine learning techniques.\n",
      "\n",
      "row 45: Attention is spread across multiple tokens, primarily focusing on the discussion of powerful machine intelligence being created in this century and the consequences of scaling up current machine learning techniques. The highest attention scores are associated with tokens indicating the possibility of systems being deceptive.\n",
      "\n",
      "row 10: Attention is mostly focused on tokens discussing the belief in the creation of powerful machine intelligence, particularly emphasizing the ideas of significantly and superhuman capabilities. The highest attention scores are observed for tokens indicating the strong belief in this possibility.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 [0, 9, 20, 15, 3141, 89, 582, 50276, 14644, 9, 20, 15, 2950, 89, 582, 50276, 936, 9, 19, 15, 2904, 89, 582, 50276, 1257, 9, 18, 15, 2227, 89, 582, 50276, 2520, 9, 18, 15, 2526, 89, 582, 50276, 1439, 9, 18, 15, 1630, 89, 582, 50276, 261, 9, 18, 15, 1012, 89, 10]\n"
     ]
    }
   ],
   "source": [
    "a = model.tokenizer(\"<|endoftext|>(3.78x),  than(3.46x),  to(2.08x),  be(1.85x),  this(1.66x),  not(1.27x),  is(1.13x)\")\n",
    "b = a[\"input_ids\"]\n",
    "print(len(b), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadData:\n",
    "    def __init__(self):\n",
    "        self.descriptions = []\n",
    "        self.samples = None\n",
    "        self.ranked_multiples = None\n",
    "        \n",
    "\n",
    "class AutoAttention:\n",
    "    def __init__(self, \n",
    "                 model: HookedTransformer, \n",
    "                 openai_model: Literal[\"gpt-3.5-turbo\", \"gpt-3.5-turbo-1106\", \"gpt-4\", \"gpt-4-32k\"],\n",
    "                 openai_api_key: str,\n",
    "                 text: List[List[str]],\n",
    "                 min_length = 10,\n",
    "                 max_length = 30\n",
    "                 ) -> None:\n",
    "        self.model = model\n",
    "        self.openai_model = openai_model\n",
    "        self.OPENAI_API_KEY = openai_api_key\n",
    "        self.heads = {layer_idx: {head_idx: HeadData() for head_idx in range(self.model.cfg.n_heads)} for layer_idx in range(self.model.cfg.n_layers)}\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.text = text\n",
    "        self.str_tokens = None\n",
    "        self.input_ids = None\n",
    "        self.cache = None\n",
    "        self.pad_token_id = model.tokenizer.pad_token_id\n",
    "        self.prompt_prefix = (\"Here is a row by row break down of which tokens were attended to for each individual generation step for a transformer. \"\n",
    "                              \"Each row is labeled with its index and is of the format for each row: `{text}\\\\n{{token} {multiple} for token, multiple in above_avg_tokens}`. \"\n",
    "                              \"Notably, the tokens with above average attention are sorted by their score's multiple relative to the average attention score for that row, \"\n",
    "                              \"and this multiple is included next to it, i.e. `tokenX 2, tokenY 1.5` etc.\")\n",
    "        self.prompt_suffix = (\"Please generate a response that summarizes what this attention head pays attention to, \"\n",
    "                             \"generalizing from the specific contexts for each example. Be as specific as possible, considering all possible \"\n",
    "                             \"reasons certain tokens may have been attended to more than others, including the position of \"\n",
    "                             \"the tokens, what the tokens themselves represent, the attention scores, and how these all interact. \"\n",
    "                             \"Do NOT explain how attention heads or transformers work or include highly general information. \"\n",
    "                             \"Limit your response to 1 paragraph.\")\n",
    "    \n",
    "    def _create_cache(self):\n",
    "        input_features = model.tokenizer(self.text, padding=True, return_tensors=\"pt\")\n",
    "        input_ids = input_features[\"input_ids\"]\n",
    "        if input_ids.shape[1] > self.max_length:\n",
    "            input_ids = input_ids[:, :self.max_length]\n",
    "        decoded = model.tokenizer.batch_decode(input_ids)\n",
    "        str_tokens = model.to_str_tokens(decoded, prepend_bos=False)\n",
    "        _, cache = model.run_with_cache(input_ids)\n",
    "        \n",
    "        self.str_tokens = str_tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.cache = cache\n",
    "        \n",
    "    def create_samples(self, head=0, layer=0):\n",
    "        print(f\"Creating new samples for layer {layer} head {head}\")\n",
    "        if self.cache is None:\n",
    "            self._create_cache()\n",
    "        # Calculate the average score multiples\n",
    "        attention_patterns = self.cache[\"pattern\", layer][:, head, ...]\n",
    "        avg_scores = 1 / t.arange(1, attention_patterns.shape[1] + 1).float().to(attention_patterns.device)\n",
    "        avg_scores = avg_scores.unsqueeze(-1)\n",
    "        attention_multiples = attention_patterns / avg_scores\n",
    "        \n",
    "        # Create the samples of [((seq_idx, length_idx), score_multiples1), ...]\n",
    "        samples = []\n",
    "        for seq_idx in range(attention_patterns.shape[0]):\n",
    "            seq_ids = self.input_ids[seq_idx].tolist()\n",
    "            pad_token_idx = seq_ids.index(self.pad_token_id) if self.pad_token_id in seq_ids else float('inf')\n",
    "            for length_idx in range(self.min_length, min(pad_token_idx, attention_patterns.shape[1])):\n",
    "                if length_idx >= pad_token_idx: \n",
    "                    break\n",
    "                str_tokens = self.str_tokens[seq_idx][:length_idx+1]\n",
    "                score_multiples = attention_multiples[seq_idx, length_idx, :length_idx+1].tolist()\n",
    "                score_multiples = [(str_token, round(multiple, 1)) for str_token, multiple in zip(str_tokens, score_multiples) if multiple > 1]\n",
    "                score_multiples.sort(key=lambda x: x[1])\n",
    "                samples.append(((seq_idx, length_idx), score_multiples))\n",
    "\n",
    "        head_data = self.heads[layer][head]\n",
    "        head_data.samples = samples\n",
    "        head_data.ranked_multiples = None\n",
    "    \n",
    "    def get_head_samples(self, head=0, layer=0, num_samples=10):\n",
    "        head_data = self.heads[layer][head]\n",
    "        if head_data.samples is None:\n",
    "            self.create_samples(head=head, layer=layer)\n",
    "        samples = head_data.samples\n",
    "        sample_indices = random.sample(range(len(samples)), num_samples)\n",
    "        rows = []\n",
    "        for i in sample_indices:\n",
    "            (seq_idx, length_idx), score_multiples = samples[i]\n",
    "            token_ids = self.input_ids[seq_idx][:length_idx+1]\n",
    "            text = self.model.tokenizer.decode(token_ids)\n",
    "            above_avg_tokens = \", \".join([f\"{token} {multiple}\" for token, multiple in score_multiples])\n",
    "            row = f\"{text}\\n{above_avg_tokens}\\n\"\n",
    "            rows.append(row)\n",
    "        \n",
    "        return \"\\n\".join(rows)\n",
    "    \n",
    "    def describe_head(self, head=0, layer=0, num_samples=10, custom_prompt=None):\n",
    "        # Retrieve samples\n",
    "        head_data = self.heads[layer][head]\n",
    "        if head_data.samples is None:\n",
    "            self.create_samples(head=head, layer=layer)\n",
    "        samples = self.get_head_samples(head=head, layer=layer, num_samples=num_samples)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"{self.prompt_prefix}\\n{samples}\"\n",
    "        if custom_prompt:\n",
    "            prompt += f\"\\n{custom_prompt}\"\n",
    "        prompt += f\"\\n{self.prompt_suffix}\"\n",
    "        \n",
    "        # Send prompt and save result\n",
    "        print(f\"Making API call to {self.openai_model}...\")\n",
    "        description = self._prompt_gpt(prompt)\n",
    "        head_data.descriptions.append(description)\n",
    "        return prompt, description\n",
    "        \n",
    "    \n",
    "    def _prompt_gpt(self, prompt: str) -> str:\n",
    "        content = \"You are a helpful mechanistic interpretability researcher who is an expert in analyzing attention patterns\"\n",
    "        response = openai.ChatCompletion.create(model=self.openai_model, messages=[{\"role\": \"system\", \"content\": content}, {\"role\": \"user\", \"content\": prompt}])\n",
    "        response_content = response['choices'][0]['message']['content']\n",
    "        return response_content\n",
    "    \n",
    "    def _create_ranked_multiples(self, layer=0, head=0):\n",
    "        head_data = self.heads[layer][head]\n",
    "        if head_data.samples is None:\n",
    "            self.create_samples(head=head, layer=layer)\n",
    "    \n",
    "        ranked_multiples = []\n",
    "        for sample in head_data.samples:\n",
    "            (seq_idx, length_idx), multiples = sample\n",
    "            for (token, multiple) in multiples:\n",
    "                str_toks = self.str_tokens[seq_idx][:length_idx+1]\n",
    "                attention_pattern = self.cache[\"pattern\", layer][seq_idx, head, length_idx, :]\n",
    "                ranking = [token, multiple, str_toks, attention_pattern]\n",
    "                ranked_multiples.append(ranking)\n",
    "                \n",
    "        ranked_multiples.sort(key=lambda x: x[1], reverse=True)\n",
    "        head_data.ranked_multiples = ranked_multiples\n",
    "    \n",
    "    def get_ranked_multiples(self, layer=0, head=0, str_token=None, num_multiples=10):\n",
    "        head_data = self.heads[layer][head]\n",
    "        if head_data.ranked_multiples is None:\n",
    "            self._create_ranked_multiples(layer=0, head=0)\n",
    "        \n",
    "        # Optionally only retrieve certain tokens\n",
    "        if str_token is not None:\n",
    "            count = 0\n",
    "            ret = []\n",
    "            for multiple in head_data.ranked_multiples:\n",
    "                ranked_str_token, *_ = multiple\n",
    "                if str_token == ranked_str_token:\n",
    "                    ret.append(multiple)\n",
    "                    count += 1\n",
    "                if count >= num_multiples:\n",
    "                    return ret\n",
    "        \n",
    "        return head_data.ranked_multiples[:num_multiples]\n",
    "    \n",
    "    def get_random_multiples(self, layer=0, head=0, str_token=None, num_multiples=10):\n",
    "        head_data = self.heads[layer][head]\n",
    "        if head_data.ranked_multiples is None:\n",
    "            self._create_ranked_multiples(layer=0, head=0)\n",
    "        \n",
    "        indices = random.sample(range(len(head_data.ranked_multiples)), num_multiples)\n",
    "        return [head_data.ranked_multiples[i] for i in indices]\n",
    "    \n",
    "\n",
    "\n",
    "auto_attention = AutoAttention(model=model, \n",
    "                               text=text[:5], \n",
    "                               openai_model=\"gpt-3.5-turbo-1106\", \n",
    "                               openai_api_key=OPENAI_API_KEY\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new samples for layer 0 head 0\n"
     ]
    }
   ],
   "source": [
    "# auto_attention.heads[0][0].samples[10]\n",
    "\n",
    "ranked_multiples = auto_attention.get_ranked_multiples(num_multiples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' D', 1.5, ['New', ' drunk', '-', 'driving', ' law', ' cracks', ' down', ' on', ' 3', 'rd', ' D', 'UI', ' LE'], tensor([0.34, 0.02, 0.02, 0.03, 0.05, 0.07, 0.02, 0.09, 0.02, 0.04, 0.11, 0.08, 0.10, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
      "        0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], device='mps:0')]\n",
      "['A', 3.8, ['A', ' magazine', ' supplement', ' with', ' an', ' image', ' of', ' Ad', 'olf', ' Hitler', ' and', ' the', ' title', \" '\", 'The', ' Un', 'readable', ' Book', \"'\", ' is', ' pictured', ' in', ' Berlin', '.'], tensor([0.16, 0.03, 0.01, 0.00, 0.00, 0.01, 0.00, 0.01, 0.01, 0.09, 0.00, 0.00, 0.04, 0.00, 0.01, 0.04, 0.07, 0.08,\n",
      "        0.00, 0.07, 0.20, 0.01, 0.14, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], device='mps:0')]\n"
     ]
    }
   ],
   "source": [
    "# code to display the ranked multiples\n",
    "test = auto_attention.get_random_multiples(num_multiples=10)\n",
    "print(test[0])\n",
    "print(test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_head(self, head=None, layer=None, samples=None, description=None):\n",
    "    head = self.heads[layer][head]\n",
    "    head.samples = samples\n",
    "    head.description = description\n",
    "    \n",
    "auto_attention._update_head = _update_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# pad_indices = (auto_attention.input_ids[0] == auto_attention.model.tokenizer.pad_token_id).nonzero(as_tuple=True)\n",
    "# if pad_indices[0].size(0) > 0:\n",
    "#     pad_index = pad_indices[0][0]\n",
    "# else:\n",
    "#     pad_index = None\n",
    "l = auto_attention.input_ids[0].tolist()\n",
    "p= auto_attention.model.tokenizer.pad_token_id\n",
    "a = l.index(p) if p in l else None\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc multiples in tensors, then move to python for storing with str_tokens\n",
    "\n",
    "How to store \"max activating\" examples? I want to be able to retrieve the original text, I want to be able to see it on a per token basis, and I want to be able to see it on an absolute magnitude basis. Might have to create a new list on the fly and sort it?\n",
    "Eh, just recalculate every time. Those might be kind of fun to see the results for, and you could visualize it.\n",
    "\n",
    "No gini coefficient though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
